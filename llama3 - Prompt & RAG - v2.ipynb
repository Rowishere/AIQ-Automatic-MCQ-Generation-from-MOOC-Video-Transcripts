{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcf012e-5d4f-47d3-8138-29c9384137b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b1880a-4a04-4c8f-b61c-fa4826a23fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from pyngrok import ngrok\n",
    "import json\n",
    "import uvicorn\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ca5635-a31f-4cf3-8a6e-cd5d5060761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request and response models\n",
    "class PromptRequest(BaseModel):\n",
    "    num_mcq: int\n",
    "    num_distractor: int\n",
    "    subject: str\n",
    "    level: str\n",
    "    trans: str  \n",
    "    outcomes_ids: List[str]\n",
    "\n",
    "class MCQResponse(BaseModel):\n",
    "    num_mcq: int\n",
    "    question: str\n",
    "    answer: str\n",
    "    distractors: List[str]\n",
    "\n",
    "class Response(BaseModel):\n",
    "    mcqs_response: List[MCQResponse]\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338679b6-b4e4-4e06-8464-ed3cf93de7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mcqs_from_transcript(transcript: str, topic: str, num_mcqs: int, num_distractors: int, difficulty: str, descriptions: str) -> Response:\n",
    "    chat_completion = groq.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an MCQ generator that outputs multiple-choice questions in JSON.\\n\"\n",
    "                           f\"The JSON object must use the schema: {json.dumps(Response.model_json_schema(), indent=2)}\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (f\"Generate {num_mcqs} multiple-choice questions with\"\n",
    "                            f\"{num_distractors} distractors each and difficulty level '{difficulty}', based on the following transcript:\\n\\n\"\n",
    "                            f\"{transcript} and apply these outcomes {descriptions}\"),\n",
    "            },\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "        temperature=0,\n",
    "        stream=False,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return Response.model_validate_json(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec7f6f8-494b-4c2e-a9c8-e71cbdf68561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_by_id(file_path, ids):\n",
    "    # Open and load the JSON data from the file\n",
    "    descriptions = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        data_dict = json.load(file)\n",
    "    \n",
    "    # Initialize a counter for numbering the outcomes\n",
    "    counter = 1\n",
    "    \n",
    "    # Iterate through each category in the dictionary\n",
    "    for category in data_dict:\n",
    "        # Check each item in the list for matching IDs\n",
    "        for item in data_dict[category]:\n",
    "            if item['id'] in ids:\n",
    "                # Append the outcome with its number to the list\n",
    "                descriptions.append(f\"{counter}- \\\"{item['description']}\\\"\")\n",
    "                counter += 1\n",
    "    \n",
    "    # Join all descriptions into a single string separated by spaces\n",
    "    return ' '.join(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57082cc-2fbe-4ed0-a67e-1ac25788bc72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d43d56b-7b6f-42e3-a95b-40a7a122078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from pyngrok import ngrok\n",
    "from typing import List, Dict, Union \n",
    "import pandas as pd\n",
    "import torch\n",
    "from groq import Groq\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "# from attention_sinks import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", None\n",
    ")  # this will be helpful when visualizing retriever outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c750ca3-af93-40e1-953b-ceac26377564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 8\n",
      "Emptied cache for GPU: 0\n",
      "Emptied cache for GPU: 1\n",
      "Emptied cache for GPU: 2\n",
      "Emptied cache for GPU: 3\n",
      "Emptied cache for GPU: 4\n",
      "Emptied cache for GPU: 5\n",
      "Emptied cache for GPU: 6\n",
      "Emptied cache for GPU: 7\n",
      "Reset max memory allocated and cached for all GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "# Free the GPU cache for each GPU\n",
    "for i in range(num_gpus):\n",
    "    torch.cuda.set_device(i)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Emptied cache for GPU: {i}\")\n",
    "\n",
    "# Optionally, reset all CUDA devices\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "print(\"Reset max memory allocated and cached for all GPUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e68d3eed-fcb8-48bc-acd7-02df26201c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='cleaned_dataset.csv', source_column=\"Transcript\")\n",
    "RAW_KNOWLEDGE_BASE = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b075d3b-f75d-4a57-aa02-afc9667820b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class.\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # the maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=100,  # the number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "\n",
    "docs_processed = []\n",
    "for doc in RAW_KNOWLEDGE_BASE:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76b5502-3fd8-4d8e-b85c-baacbfee344d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e86be09586b434d8933036f925ed4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-large\"\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "\n",
    "docs_processed = split_documents(\n",
    "    512,  # We choose a chunk size adapted to our model\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
    "#fig = pd.Series(lengths).hist()\n",
    "#plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0d4028-8fe2-4184-bfee-545d754eb48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0e69ac-61f8-4c50-98e7-9742d2635d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197ff009-b73e-4cba-9298-4447b6c04d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    final_prompt: str,\n",
    "    transcript: str,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 90,\n",
    "    num_docs_final: int = 30,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \n",
    "    # Gather documents with retriever\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=transcript, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = reranker.rerank(transcript, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    # Redact an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "\n",
    "    chat_completion = groq.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an MCQ generator that outputs multiple-choice questions in JSON.\\n\"\n",
    "                           f\"The JSON object must use the schema: {json.dumps(Response.model_json_schema(), indent=2)}\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (f\"{final_prompt}\"),\n",
    "            },\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "        temperature=0,\n",
    "        stream=False,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    \n",
    "    answer = Response.model_validate_json(chat_completion.choices[0].message.content)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03f353-e065-442c-989a-1b39f4769d00",
   "metadata": {},
   "source": [
    "## Fast API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8fc196b-4b6f-443a-a44e-4b4d1e64a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request and response models\n",
    "class PromptRequest(BaseModel):\n",
    "    num_mcq: int\n",
    "    num_distractor: int\n",
    "    subject: str\n",
    "    level: str\n",
    "    trans: str  \n",
    "    outcomes_ids: List[str]\n",
    "\n",
    "class MCQResponse(BaseModel):\n",
    "    num_mcq: int\n",
    "    question: str\n",
    "    answer: str\n",
    "    distractors: List[str]\n",
    "\n",
    "class Response(BaseModel):\n",
    "    mcqs_response: List[MCQResponse]\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf3a9538-1e53-427d-b00d-b78b5af0326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FastAPI endpoint\n",
    "\n",
    "@app.post(\"/generate-response-rag/\", response_model=Response)\n",
    "async def generate_response_rag(prompt_request: PromptRequest):\n",
    "\n",
    "    file_path = 'outcomes.json'\n",
    "    transcript = f\"{prompt_request.trans}\"\n",
    "    descriptions = get_description_by_id(file_path, prompt_request.outcomes_ids)\n",
    "    \n",
    "    final_prompt = f'''\n",
    "    Generate {prompt_request.num_mcq} multiple-choice questions with {prompt_request.num_distractor} distractors each and difficulty level '{prompt_request.level}', based on the following transcript:\\n\\n {transcript} and apply these outcomes {descriptions}\n",
    "    '''\n",
    "\n",
    "  \n",
    "    answer, relevant_docs = answer_with_rag(\n",
    "        final_prompt = final_prompt,\n",
    "        transcript=transcript, \n",
    "        reranker=RERANKER, \n",
    "        knowledge_index=KNOWLEDGE_VECTOR_DATABASE\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48fad45f-d304-4d7f-a4d9-2f4f5b69a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/generate-response-prompt/\", response_model=Response)\n",
    "async def generate_response_prompt(request: PromptRequest):\n",
    "    try:\n",
    "        file_path = 'outcomes.json'\n",
    "        descriptions = get_description_by_id(file_path, request.outcomes_ids)\n",
    "        mcqs_response = generate_mcqs_from_transcript(request.trans, request.subject, request.num_mcq, request.num_distractor, request.level, descriptions)\n",
    "        return mcqs_response\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error generating MCQs: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b281d-2de8-4f4c-a45a-848912bbb10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2024-06-10T14:32:54+0300 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/home/ahmed-khaled-st/.config/ngrok/ngrok.yml legacy_path=/home/ahmed-khaled-st/.ngrok2/ngrok.yml\n",
      "t=2024-06-10T14:32:54+0300 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
      "INFO:     Started server process [2663100]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8887 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrok tunnel: NgrokTunnel: \"https://fe67-45-240-51-111.ngrok-free.app\" -> \"http://localhost:8887\"\n",
      "=> Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed-khaled-st/soly/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n",
      "INFO:     45.240.51.111:0 - \"POST /generate-response-rag/ HTTP/1.1\" 200 OK\n",
      "INFO:     45.240.51.111:0 - \"POST /generate-response-prompt/ HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Connect ngrok to get tunnel\n",
    "public_url = ngrok.connect(addr=8887)\n",
    "print(\"ngrok tunnel:\", public_url)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8887)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e467e8-9ef5-42bb-b37a-a5d707d96c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e831a-f86c-4fe0-b764-50f3d9acb0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
